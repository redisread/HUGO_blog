---
title: "Scrapy 1"
date: 2020-03-04T16:35:09+08:00
description:
draft: false
hideToc: false
enableToc: true
enableTocContent: false
tocPosition: inner
author: Victor
authorEmoji: ğŸ‘»
image: https://i.loli.net/2020/03/04/k8KU5LnrW7PlCgd.png
libraries:
- 
tags:
- çˆ¬è™«
- Scrapy
series:
- çˆ¬è™«
categories:
-
---

ä½¿ç”¨Scrapyçˆ¬å–æ–‡ç« çš„ä¸€ä¸ªå°é¡¹ç›®..

<!--more-->

# Scrapy

æ¡†æ¶å›¾ï¼š

![](https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1021076823,752564480&fm=15&gp=0.jpg)



## æŠ“å–å°ç¨‹åºç¤¾åŒºæ–‡ç« 

![](https://i.loli.net/2020/03/04/NRDI6mQb1Lzphui.png)

### åˆ›å»ºçˆ¬è™«é¡¹ç›®

åˆ›å»ºé¡¹ç›®ï¼ˆé¡¹ç›®åä¸ºMyTestï¼‰

```bash
scrapy startproject MyTest
```

åˆ›å»ºçˆ¬è™«:beetle:(å…ˆè¿›å…¥åˆ°MyTestç›®å½•)

```bash
scrapy genspider -t crawl wx wxapp-union.com
```

> wxä¸ºçˆ¬è™«çš„åå­—ï¼Œwxapp-union.comä¸ºçˆ¬å–çš„åŸŸåï¼Œä½¿ç”¨äº†æ¨¡æ¿crawl

### å®šä¹‰çˆ¬å–çš„æ•°æ®ç»“æ„

çˆ¬å–çš„æ•°æ®ç»“æ„ç±»ç»§æ‰¿Itemç±»ï¼Œåœ¨items.pyæ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹æ˜¯è®¾ç½®éœ€è¦çˆ¬å–çš„æ•°æ®ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬:æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€è®¿é—®è€…ã€å‰è¨€ã€æ­£æ–‡ã€‚

```python
from scrapy import Item,Field

# å®šä¹‰æ–‡ç« æ•°æ®ç»“æ„
class ArticleItem(Item):
    title = Field()
    author = Field()
    _time = Field()
    visitors = Field()
    pre_talk = Field()
    article_content = Field()
```



### ç¼–å†™çˆ¬è™«è§„åˆ™ä¸è§£æè§„åˆ™

> çˆ¬è™«çš„çˆ¬å–ç½‘é¡µçš„é“¾æ¥çš„è§„åˆ™å’Œè§£æé¡µé¢çš„è§„åˆ™éƒ½æ˜¯åœ¨æ–°å»ºçš„spideræ–‡ä»¶ä¸­çš„ç±»ä¸­ï¼Œä¹Ÿå³åœ¨wx.pyä¸­

ç¼–å†™çš„spiderç±»å¦‚ä¸‹:

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from MyTest.items import ArticleItem

class WxSpider(CrawlSpider):
    name = 'wx'
    allowed_domains = ['wxapp-union.com']
    start_urls = ['http://www.wxapp-union.com/portal.php?mod=list&catid=2&page=255']

    rules = (
        Rule(LinkExtractor(allow=r'.+mod=list&catid=2&page=\d'), follow=True),    Rule(LinkExtractor(allow=r'.+article-.+\.html'),callback="parse_item",follow=False)
    )

    def parse_item(self, response):
        title = response.xpath('//h1[@class="ph"]/text()').get()
        author = response.xpath('//p[@class="authors"]//a').get()
        _time = response.xpath('//span[@class="time"]/text()').get()
        visitors = response.xpath('//div[contains(@class,"focus_num")]//a/text()').get()
        pre_talk = response.xpath('//div[@class="blockquote"]//p/text()').get()
        article_content = response.xpath('//td[@id="article_content"]').get()
        item = ArticleItem(title=title,author=author,_time=_time,visitors=visitors,pre_talk=pre_talk,article_content=article_content)
        print('*'*40)
        print(title)
        print('*'*40)
        return  item
```

* é¦–å…ˆruleså®šä¹‰äº†çˆ¬å–é“¾æ¥è§„åˆ™ï¼Œæœ‰ä¸¤ä¸ªè§„åˆ™ï¼Œç¬¬ä¸€ä¸ªè§„åˆ™æ˜¯çˆ¬å–é¡µé¢çš„é“¾æ¥ï¼Œæ¯ä¸€é¡µæœ‰å¤šä¸ªæ–‡ç« çš„é“¾æ¥ï¼Œè€Œç¬¬äºŒä¸ªè§„åˆ™åˆ™æ˜¯å®šä¹‰çˆ¬å–çš„å…·ä½“æ–‡ç« å†…å®¹çš„é“¾æ¥ã€‚
* ç¬¬ä¸€ä¸ªè§„åˆ™éœ€è¦Followï¼Œå› ä¸ºéœ€è¦æ ¹æ®æ¯ä¸€é¡µçš„å†…å®¹æŸ¥æ‰¾æ–‡ç« çš„é“¾æ¥ï¼›è€Œç¬¬äºŒä¸ªè§„åˆ™æ˜¯æ–‡ç« é“¾æ¥ï¼Œæ•…ä¸éœ€è¦ç»§ç»­Follow
* ç¬¬ä¸€ä¸ªé¡µé¢é“¾æ¥è§„åˆ™ä¸éœ€è¦å›è°ƒå‡½æ•°ï¼Œå› ä¸ºä¸éœ€è¦è§£æï¼Œåªéœ€è¦è·å–æ–‡ç« é“¾æ¥ï¼›ç¬¬äºŒä¸ªæ–‡ç« é“¾æ¥è§„åˆ™åˆ™éœ€è¦è®¾ç½®å›è°ƒå‡½æ•°æ¥å¯¹è¿”å›çš„æ–‡ç« ç½‘é¡µå†…å®¹è¿›è¡Œè§£æã€‚

**parse_itemè¯´æ˜ï¼š**

> parse_itemæ˜¯è§£æé¡µé¢è¿”å›å†…å®¹çš„å‡½æ•°ï¼Œå…¶è¿”å›Itemæ•°æ®ç»“æ„ï¼Œä½¿ç”¨Xpathåˆ†åˆ«è·å–æ•°æ®ç»“æ„å„ä¸ªå…ƒç´ çš„å†…å®¹å¹¶ä¸”è¿”å›Item

### ä¿å­˜æ•°æ®

> pipelinesæ˜¯ä¸€ä¸ªæœ€åå¤„ç†Itemçš„ç®¡é“

åœ¨pipelines.pyæ–‡ä»¶ä¸­æ–°å»ºpiplelineå¯¹è¿”å›çš„Itemè¿›è¡Œå¤„ç†ï¼Œå¯ä»¥ä¿å­˜ä¸ºæ–‡ä»¶ï¼Œæˆ–è€…å­˜å‚¨åˆ°æ•°æ®åº“ã€‚

é¦–å…ˆæ–‡ä»¶ä¸­éœ€è¦å¯¼å…¥å¿…è¦çš„åº“â€˜

```python
import re	# æ­£åˆ™å¤„ç†
from html2text import HTML2Text	# å°†ç½‘é¡µè½¬åŒ–ä¸ºMarkdownæ ¼å¼
from scrapy.exporters import JsonLinesItemExporter	# è¾“å‡ºJsonæ–‡ä»¶è¾“å‡ºå™¨
from urllib.parse import urljoin	# è¡¥å…¨URLï¼Œå› ä¸ºæœ‰äº›URLåªæ˜¾ç¤ºç›¸å¯¹ä½ç½®
import pymongo	# MongoDBæ“ä½œåº“
```

#### ç¬¬ä¸€ä¸ªPipelineï¼šä¿å­˜åˆ°Jsonæ–‡ä»¶

ç¨‹åºçš„æ„é€ å‡½æ•°æ–°å»ºä¸€ä¸ªJsonæ–‡ä»¶è¾“å‡ºå™¨ï¼Œprocess_itemè¿›è¡Œæ•°æ®çš„å­˜å‚¨ï¼Œå…³é—­çš„æ—¶å€™close_spiderä¼šè°ƒç”¨å…³é—­æ–‡ä»¶

```python
# å­˜å‚¨åˆ°Jsonæ–‡ä»¶ä¸­
class JsonPipeline(object):
    def __init__(self):
        self.f = open('wxjc.json','wb')
        self.exporter = JsonLinesItemExporter(self.f,
        ensure_ascii=False,encoding="utf-8")

    def process_item(self, item, spider):
        # å°†å†…å®¹è½¬åŒ–ä¸ºMarkDownæ ¼å¼
        item['article_content'] = convert_md(item['article_content'])
        self.exporter.export_item(item)
        return item
    
    def close_spider(self,spider):
        self.f.close()
```

#### ç¬¬äºŒä¸ªPipelineï¼šä¿å­˜åˆ°Markdownæ–‡ä»¶

æ–¹æ³•ä¸ç¬¬ä¸€å‘Pipelineç±»ä¼¼ï¼Œåªæ˜¯å†™æ–‡ä»¶ä½¿ç”¨æœ€ç®€å•çš„è¿½åŠ æ–¹å¼

```python
# å†™å…¥Markdown
class MDPipeline(object):
    def __init__(self):
        self.f = open('wx_teaches.md','a',encoding='utf-8')

    def process_item(self,item,spider):
        if self.f:
            self.f.write('\n')
            self.f.write("# " + item['title'] + '\n')
            header_info = "ä½œè€…:{}      å‘å¸ƒæ—¶é—´:{}      Visitors:{}\n".format(item['author'],item['_time'],item['visitors'])
            self.f.write(header_info)
            self.f.write('> ' + item['pre_talk'] + '\n')
            self.f.write(item['article_content'])
        return item
    
    def close_spider(self,spider):
        self.f.close()
```

#### ç¬¬ä¸‰ä¸ªPilelineï¼šä¿å­˜åˆ°MongoDB

å…¶ä¸­ä½¿ç”¨äº†ç±»æ–¹æ³•è£…é¥°å™¨@classmethod,æ„æ€å°±æ˜¯ç›´æ¥ç”¨ç±»åè°ƒç”¨è¯¥å‡½æ•°ï¼Œå°±èƒ½å¤Ÿç›´æ¥è¿”å›ä¸€ä¸ªMongoPipelineç±»äº†ï¼Œè¿˜å®šä¹‰äº†æ‰“å¼€spiderä¸å…³é—­spiderçš„æ“ä½œï¼Œå°±æ˜¯è¿æ¥æ•°æ®åº“ä¸å…³é—­æ•°æ®åº“

```python
# å­˜å‚¨åˆ°MongoDBæ•°æ®åº“
class MongoPipeline(object):
    def __init__(self,mongo_uri,mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    @classmethod
    def from_crawler(cls,crawler):
        return cls(mongo_uri = crawler.settings.get('MONGO_URI'),
                    mongo_db = crawler.settings.get('MONGO_DB'))
    
    def open_spider(self,spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db =self.client[self.mongo_db]
    
    def process_item(self,item,spider):
        name = item.__class__.__name__
        # <a href=\"space-uid-17761.html\">Rolan</a> 
        item['author'] = re.search('<a.*?>(.*?)</a>',item['author']).group(1)
        self.db[name].insert(dict(item))
        return item
    
    def close_spider(self,spider):
        self.client.close()
```

> æœ€åéœ€è¦åœ¨settings.pyä¸­æ·»åŠ å¦‚ä¸‹å­—æ®µ:
>
> ```python
> MONGO_URI = 'localhost'
> MONGO_DB = 'WX'
> ```

#### æœ€åéœ€è¦åœ¨settings.pyä¸­æ·»åŠ å¦‚ä¸‹å­—æ®µ

```python
ITEM_PIPELINES = {
   'MyTest.pipelines.JsonPipeline': 300,
   'MyTest.pipelines.MDPipeline': 301,
   'MyTest.pipelines.MongoPipeline': 400,
}
# ä¿®æ”¹ä¸ºFalse
ROBOTSTXT_OBEY = False
# è®¾ç½®å»¶è¿Ÿ1s
DOWNLOAD_DELAY = 1
```

### å¼€å§‹çˆ¬å–

å¯ä»¥åœ¨é¡¹ç›®ç›®å½•ä¸­æ–°å»ºä¸€ä¸ªè„šæœ¬start.pyï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼Œè‡ªåŠ¨è¿è¡Œè„šæœ¬

```python
from scrapy import cmdline
cmdline.execute('scrapy crawl test'.split(' '))
```

### çˆ¬å–ç»“æœ

#### Jsonç»“æœ

![](https://i.loli.net/2020/03/04/Bfi59qDo8hreuEA.png)

#### Markdownç»“æœ

Markdownæ–‡ä»¶ç”±äºå¤ªå¤§äº†ä½¿ç”¨Markdownæ–‡ä»¶æ‰“ä¸å¼€ï¼Œåªå¥½ä½¿ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€

![](https://i.loli.net/2020/03/04/3Fux6YOIPRcTLVb.png)

#### MongoDBç»“æœ

![](https://i.loli.net/2020/03/04/Ub2RnpkJYMcEV9H.png)