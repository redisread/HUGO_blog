---
title: "MXNetå›é¡¾"
date: 2020-02-23T15:49:10+08:00
description:
draft: false
hideToc: false
enableToc: true
enableTocContent: false
tocPosition: inner
author: Victor
authorEmoji: ğŸ‘»
image: https://i.loli.net/2020/02/23/NASIuoQYPCpW18x.png
libraries:
- katex
- mathjax
tags:
- python
- MXNet
- æ·±åº¦å­¦ä¹ 
- liner
series:
- æ·±åº¦å­¦ä¹ 
categories:
-
---

ä½¿ç”¨MXNetçš„å¥½å¤„ä½ æ°¸è¿œæƒ³è±¡ä¸åˆ°ã€‚:accept:

<!--more-->

### **æœ¬åœ°ç¯å¢ƒæ­å»ºæ•™ç¨‹**

> å‚è€ƒ:
>
> https://discuss.gluon.ai/t/topic/13576?u=bigbigwolf-ai



### **èŒƒæ•°**

L0èŒƒæ•°ï¼šæŒ‡å‘é‡ä¸­é0å…ƒç´ çš„ä¸ªæ•°ã€‚ï¼ˆéš¾ä¼˜åŒ–æ±‚è§£ï¼‰

L1èŒƒæ•°ï¼šæŒ‡å‘é‡ä¸­å„ä¸ªå…ƒç´ çš„ç»å¯¹å€¼ä¹‹å’Œ

L2èŒƒæ•°ï¼šæŒ‡å‘é‡å„å…ƒç´ çš„å¹³æ–¹å’Œç„¶åæ±‚å¹³æ–¹æ ¹

è®¾$n$ç»´å‘é‡$x$ä¸­çš„å…ƒç´ ä¸º$x_1, \ldots, x_n$ã€‚å‘é‡$x$çš„$L_{p}$èŒƒæ•°ä¸º:
$$
\|\boldsymbol{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
$$


$L_{1}$èŒƒæ•°ï¼š
$$
\|\boldsymbol{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.
$$
$L_{2}$èŒƒæ•°ï¼š
$$
\|\boldsymbol{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}.
$$


è®¾$X$æ˜¯ä¸€ä¸ª$m$è¡Œ$n$åˆ—çŸ©é˜µã€‚çŸ©é˜µ$X$çš„FrobeniusèŒƒæ•°ä¸ºè¯¥çŸ©é˜µå…ƒç´ å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ï¼š
$$
\|\boldsymbol{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2},
$$

### **æŸ¥é˜…æ–‡æ¡£**


```python
from mxnet import nd
print(dir(nd.random))
```

    ['NDArray', '_Null', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_internal', '_random_helper', 'current_context', 'exponential', 'exponential_like', 'gamma', 'gamma_like', 'generalized_negative_binomial', 'generalized_negative_binomial_like', 'multinomial', 'negative_binomial', 'negative_binomial_like', 'normal', 'normal_like', 'numeric_types', 'poisson', 'poisson_like', 'randint', 'randn', 'shuffle', 'uniform', 'uniform_like']

helpå‡½æ•°å¯ä»¥æŸ¥è¯¢å…·ä½“çš„å‡½æ•°ä½œç”¨åŠç”¨æ³•

```python
help(nd.ones_like)
```

    Help on function ones_like:
    
    ones_like(data=None, out=None, name=None, **kwargs)
        Return an array of ones with the same shape and type
        as the input array.
        
        Examples::
        
          x = [[ 0.,  0.,  0.],
               [ 0.,  0.,  0.]]
        
          ones_like(x) = [[ 1.,  1.,  1.],
                          [ 1,  1.,  1.]]
                          Parameters
        ----------
        data : NDArray
            The input
        
        out : NDArray, optional
            The output NDArray to hold the result.
        
        Returns
        -------
        out : NDArray or list of NDArrays
            The output of this function.

### **çº¿æ€§å›å½’**

å¯¼å…¥å¿…è¦çš„åº“


```python
%matplotlib inline
from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd, nd
import random
```

ç”Ÿæˆæ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªä¾‹å­è¾“å…¥æ•°æ®ä¸ªæ•°ä¸º2ï¼Œæœ‰1000ä¸ªæ•°æ®


```python
num_inputs = 2
num_examples = 1000
true_w = nd.array([2, -3.4])
true_b = nd.array([4.2])
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
labels = nd.dot(true_w,features.T) + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)
```

æŸ¥çœ‹æ•°æ®


```python
features[0], labels[0]
```


    (
     [ 0.28752208 -0.04466231]
     <NDArray 2 @cpu(0)>,
     
     [4.927063]
     <NDArray 1 @cpu(0)>)

å®šä¹‰ç›¸å…³å‡½æ•°


```python
def use_svg_display():
    # ç”¨çŸ¢é‡å›¾æ˜¾ç¤º
    display.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # è®¾ç½®å›¾çš„å°ºå¯¸
    plt.rcParams['figure.figsize'] = figsize

set_figsize()
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1);  # åŠ åˆ†å·åªæ˜¾ç¤ºå›¾
```


![svg](../../../../../googleä¸‹è½½/test/output_39_0.svg)


data_iterå‡½æ•°ä½œç”¨:

1. æ‰°ä¹±è¯»å–é¡ºåºï¼Œä½¿å¾—è¯»å–éšæœº
2. æŒ‰Batch_sizeåˆ†æ®µå–æ•°æ®ï¼Œéœ€è¦åˆ¤æ–­æ˜¯å¦åˆ°ç»“å°¾ï¼Œä½¿ç”¨yieldæ„å»ºç”Ÿæˆå™¨èŠ‚çœå†…å­˜


```python
# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # æ ·æœ¬çš„è¯»å–é¡ºåºæ˜¯éšæœºçš„
    for i in range(0, num_examples, batch_size):
        j = nd.array(indices[i: min(i + batch_size, num_examples)])
        yield features.take(j), labels.take(j)  # takeå‡½æ•°æ ¹æ®ç´¢å¼•è¿”å›å¯¹åº”å…ƒç´ 
```


```python
batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, y)
    break
```


    [[-0.65439206  0.74410725]
     [ 0.69013244 -0.6483847 ]
     [-0.59409887  0.3589477 ]
     [-0.47491348  0.6438462 ]
     [ 0.5074032   0.42834154]
     [-0.18589513 -0.21707669]
     [ 0.70281196 -1.3320632 ]
     [ 1.2072632   1.6909351 ]
     [-0.17264698 -1.5742793 ]
     [-1.6516455  -0.29966688]]
    <NDArray 10x2 @cpu(0)> 
    [ 0.37379816  7.7938933   1.7758217   1.0414512   3.743439    4.5605783
     10.148926    0.84148276  9.19984     1.9295483 ]
    <NDArray 10 @cpu(0)>


åˆå§‹åŒ–


```python
w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))
b = nd.zeros(shape=(1,))
```

æ·»åŠ ä¿å­˜æ¢¯åº¦çš„ç©ºé—´


```python
w.attach_grad()
b.attach_grad()
```


```python
def linreg(X, w, b):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    return nd.dot(X, w) + b
```


```python
def squared_loss(y_hat, y):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```


```python
def sgd(params, lr, batch_size):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    for param in params:
        param[:] = param - lr * param.grad / batch_size
```

å¼€å§‹è®­ç»ƒ


```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):  # è®­ç»ƒæ¨¡å‹ä¸€å…±éœ€è¦num_epochsä¸ªè¿­ä»£å‘¨æœŸ
    # åœ¨æ¯ä¸€ä¸ªè¿­ä»£å‘¨æœŸä¸­ï¼Œä¼šä½¿ç”¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ä¸€æ¬¡ï¼ˆå‡è®¾æ ·æœ¬æ•°èƒ½å¤Ÿè¢«æ‰¹é‡å¤§å°æ•´é™¤ï¼‰ã€‚X
    # å’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
    for X, y in data_iter(batch_size, features, labels):
        with autograd.record():
            l = loss(net(X, w, b), y)  # læ˜¯æœ‰å…³å°æ‰¹é‡Xå’Œyçš„æŸå¤±
        l.backward()  # å°æ‰¹é‡çš„æŸå¤±å¯¹æ¨¡å‹å‚æ•°æ±‚æ¢¯åº¦
        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£æ¨¡å‹å‚æ•°
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))
```

    epoch 1, loss 0.040809
    epoch 2, loss 0.000157
    epoch 3, loss 0.000051

å¯¹æ¯”

```python
true_w, w
```


    (
     [ 2.  -3.4]
     <NDArray 2 @cpu(0)>,
     
     [[ 1.9991481]
      [-3.3992586]]
     <NDArray 2x1 @cpu(0)>)


```python
true_b, b
```


    (
     [4.2]
     <NDArray 1 @cpu(0)>,
     
     [4.19921]
     <NDArray 1 @cpu(0)>)
